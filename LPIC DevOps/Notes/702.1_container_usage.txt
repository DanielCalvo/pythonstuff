702.1 Container Usage (weight: 7)
Weight: 7

Description: Candidates should be able to build, share and operate Docker containers. This includes creating Dockerfiles, using a Docker registry, creating and interacting with containers as well as connecting containers to networks and storage volumes. This objective covers the feature set of Docker version 17.06 or later.

Key Knowledge Areas:

- Understand the Docker architecture <- 1 ----- DOCKER ARCH -----
- Use existing Docker images from a Docker registry <- 2 ----- DOCKER REGISTRY -----
- Create Dockerfiles and build images from Dockerfiles <- 3 ----- DOCKERFILES -----
- Upload images to a Docker registry <- 4 ----- IMAGEUPLOAD -----
- Operate and access Docker containers <- 5 ----- CONTAINER OPS -----
- Connect container to Docker networks <- 6 ----- DOCKERNET -----
- Use Docker volumes for shared and persistent container storage <- 7 ----- DOCKERVOLUMES -----

The following is a partial list of the used files, terms and utilities:
- docker <- 8 ----- DOCKERCMD -----
- Dockerfile <- 3 ----- DOCKERFILES -----
- .dockerignore <- 9 ----- DOCKERIGNORE -----


===== YOUR NOTES =====

Suggested next action item: Continue with the mastering docker book. Many of the items that are unclear to you may become easier to understand as you expand your knowledge on the subject. You can skip parts that touch on points that are not particularly interesting for now.

- Mastering docker: Page 114

Once you reach an interesting part of the book, implement the things you have learned!

After finishing that book, you may want to look into other books.

Oooh! This one looks interesting: https://www.amazon.com/Using-Docker-Developing-Deploying-Containers/dp/1491915765/

There might also be an interesting book on docker networking out there!



Symfony demo:

docker image build . --tag symfonydemo:v1

composer create-project symfony/symfony-demo

Check out what to install
docker exec -i -t 32cb974d0678 /bin/bash



docker run -it <image> /bin/bash

docker run -d dani:salt_master


Things you've read:


ON WHEN DOCKER MIGHT NOT THE BEST FIT
https://blog.abevoelker.com/why-i-dont-use-docker-much-anymore/ <- Interesting article on some shortcomings of Docker and things that it may not do well. A bit old, but useful.
http://etherealmind.com/cattle-vs-kittens-on-cloud-platforms-no-one-hears-the-kittens-dying/ <- Short article on cattle vs kittens as far as applications go
https://www.loggly.com/blog/top-5-docker-logging-methods-to-fit-your-container-deployment-strategy/ <- Some information on how to setup logging on containers
https://www.linode.com/docs/applications/containers/when-and-why-to-use-docker/ <- Touches on a few points on when containers might not be the best idea also
https://www.ctl.io/developers/blog/post/what-is-docker-and-when-to-use-it/ <- Interesting assorted info on Docker

On Docker's relevancy:
https://www.zdnet.com/article/what-is-docker-and-why-is-it-so-darn-popular/

On running all this stuff in production:
https://blog.cloud66.com/9-crtitical-decisions-needed-to-run-docker-in-production/
https://searchitoperations.techtarget.com/tip/Set-up-Docker-container-networking-for-real-production-workloads
https://searchitoperations.techtarget.com/news/450411882/Docker-storage-opens-new-frontiers-in-quest-for-enterprise-adoption
https://searchitoperations.techtarget.com/opinion/When-working-with-container-technology-proceed-with-caution
https://blog.docker.com/2016/12/understanding-docker-networking-drivers-use-cases/ <- Very good one explaining networks
===== YOUR NOTES =====

1 ----- DOCKER ARCH -----

https://docs.docker.com/engine/docker-overview/
https://www.docker.com/what-container

BASIC THINGS TO CONSIDER:

- Docker provides the ability to package and run an application in a loosely isolated environment called container.
- Containers are lightweight. They are not a virtual machine and do not need a hypervisor. They run directly on the host machine's kernel.

The above gives you flexibility. You can develop your application and it's supporting components by using containers. The container then becomes the unit for distributing and testing your application. When all is ready, you can deploy this container on the production environment. (Of course, that's the theory. Reality often times more complex)

DOCKER ADVANTAGES A SALESMAN WOULD TELL YOU ABOUT:

Docker allows for "highly portable workflows" (woah). A given container can run containers on your laptop, on a virtual machine, on the testing environment or on the production environment, being scaled up or down depending on the workload that you have (your laptop for local development, not so much, the production environment, all of it!)

Docker containers are lightweight as opposed to virtual machines. They fit high density environments well and are useful for small and medium deployments where you need efficient use of resources.

THE DOCKER ENGINE:

The docker engine is a client server application with certain components.

- The server. aka the docker daemon. (dockerd). This daemon creates and manages objects such as images, containers, networks and volumes.
- The client. In this case, the docker image and container command line interface.
- A REST API that gives an interface for programs to talk to the docker daemon (the server, aka the dockerd process) and tell it what to do.

In other words: The client talks to to the server through an API. You can write your own programs/scripts to talk to the docker API and do docker things.

The client and the system can run at the same system, or you can connect to a remote docker daemon with the client over the API.

DOCKER OBJECTS:

IMAGES

Per the docker Doc: "read only template with instructions for creating a docker container".

In other words, an image is the contents you start your docker container with. An image can be even based on another, already existing image, which is what you do when create a Dockerfile that for example: fetches the default Ubuntu image, installs apache and copies some configs into the docker container.

When you change a Dockerfile and rebuild the image, only the parts that are changed are factored in into the image. This helps make docker lightweight.

CONTAINERS

Runable instance of an image. Can be started, stopped, moved and deleted with the API/CLI. Can connect to networks, have storage attached and make a lot of other things.
By default, relatively isolated from it's host machine and other containers, though this can be changed.
A container is defined by its image AND any other configuration options you provide to it when you start it. When a container is removed, any unsaved changes to it's state that are not stored in  permanent storage are lost


This command launches an ubuntu container:

docker run -i -t ubuntu /bin/bash

What this does, step by step:

1. If you do not have the ubuntu image stored locally, it is pulled from your configured registry. This would be the same as running docker pull ubuntu manually.
2. Docker creates a new container, just as if you would have typed docker container create
3. Docker creates a RW filesystem for the container. This allows the running container to modify file in it's filesystem.
4. Docker creates a network interface and plugs it into the container. Since no options were specified, this interface is connected to the default docker network.
5. Docker starts the container and runs /bin/bash.
6. When you type exit and exit bash, the container stops, but is not removed.


SERVICES

Services allow you to scale containers across multiple daemons (aka machines), which all work together as a swarm with managers and workers.
Each member of the swarm is a docker daemon (very likely a machine) and these daemons all communicate using the Docker API.
A service allows you define a desired state of these containers, such as how many of them do you want to have.
By default, a service is load balanced across all worker nodes. To the consumer, the service appears as a single entity.


Not covered by the initial doc:
VOLUMES
NETWORKS
LOCAL DOCKER DAEMONS
SWARM NODES
SWARM SERVICES


UNDERLYING DOCKER TECH

NAMESPACES

Docker uses a technology named namespaces to isolate containers. When you launch a container, it gets it's own namespace. The container is limited and isolated by this namespace.

(assumed) There are multiple namespaces for a given container:

PID: Process Isolation
NET: Network interfaces management
IPC: Inter process communication management
MNT: Filesystem mounts management
UTS: Kernel and version identifiers (sounds cryptic!)

CONTROL GROUPS

This is a technology used by docker to limit an application to a given set of resources. This allows docker to share hardware resources and optionally enforce limits on those. For example, you could limit memory usage for a given container.


UNION FILESYSTEMS

Docker uses UnionFS to "provide the building blocks for containers". I have a feeling this is the filesystem technology used by containers.

CONTAINER FORMAT

Docker combines namespaces, control groups and UnionFS in a wrapper called container format. A library named libcontainer is used for this. In the future Docker may support further container support further container technologies such as BSD Jails or Solaris Zones

1 ----- DOCKER ARCH -----


2 ----- DOCKER REGISTRY -----


CONCEPTUALLY:
The docker registry stores Docker images. There are two public registries that docker is configured to use by default: Docker Hub and Docker Cloud.
You can also of course, run your own registry with your own images.
When you use commands such as docker pull, docker push or docker run, your images are either pulled or pushed to/from the registry, depending on what you're doing.
The Docker store also allows you to buy and sell images.

RUNNING YOUR OWN REGISTRY

Pull the registry image:
docker image pull registry

Launch it:
docker container run -d -p 5000:5000 --name testregistry registry

Upload an image to it:
docker image pull debian
docker image tag debian localhost:5000/danidebian
docker image push localhost:5000/danidebian

Delete your local image:
docker image rm localhost:5000/danitest

Pull it again!
docker image pull localhost:5000/danitest

Extra challenge: Set up this stuff correctly and across the network: https://docs.docker.com/registry/configuration/

There are also some commercially available registries from Amazon, Redhat and Quay. These fall outside of LPIC's scope.



2 ----- DOCKER REGISTRY -----



3 ----- DOCKERFILES -----

Very interesting guide:
https://docs.docker.com/develop/develop-images/dockerfile_best-practices/

Docker can build images automatically by reading instructions from a Dockerfile.

Available Dockerfile instructions

FROM
The FROM command tells Docker which image you'd like to use as your base image. Examples:
FROM debian:latest
FROM alpine:latest
FROM scratch

LABEL
The LABEL can be used to add additional information to an image. Anything from a version number to a description. This doesn't do anything to the container/image itself, it only helps with identification
LABEL maintainer="Dani"
LABEL description="This example Dockerfile installs Apache and PHP."

RUN
A commands to run on container creation. These commands are executed on top of the base imaged used for the container. Examples:
RUN apt-get update && apt-get install -y apache2 && apt-get clean && rm -rf /var/lib/apt/lists/*

COPY
Copy files from whatever source into the container upon launch. Examples:
COPY files/nginx.conf /etc/nginx/nginx.conf
COPY files/default.conf /etc/nginx/conf.d/default.conf


CMD
According to the Docker doc: "The main purpose of the CMD is to provide defaults for an executing container"



There can only be one CMD command inside a dockerfile



RUN APT-GET

EXPOSE
ENV
ADD

ENTRYPOINT
VOLUME
USER
DIR
WORKDIR
ONBUILD

NEXT STEPS:
- Create some simple dockerfiles to create a few basic Docker images and acquantaince yourself with the format of the file and the command necessary to create images. How about web and database servers?
- Have a read and go through the available commands described above (might be a bit boring though)
- Perhaps have a quick read about docker networking and bridge a few machines over your home network and see how they interact?
- Docker and salt looks interesting too! Perhaps Dockerize a salt master and see how you can run some salt states inside of it?

3 ----- DOCKERFILES -----


6 ----- DOCKERNET -----

To bridge Docker to a physical inteface, you have to use the macvlan driver>

https://docs.docker.com/network/macvlan/

docker network create -d macvlan --subnet=192.168.1.0/24 --gateway=192.168.1.1 -o parent=wlxe894f6138b06 pub_net

docker run --rm -itd --network pub_net --name my-macvlan-alpine debian:latest bash



6 ----- DOCKERNET -----

7 ----- DOCKERVOLUMES -----

https://docs.docker.com/storage/volumes

Volumes are the preferred mechanism to keep data persistent in containers. Yay for persistency!

docker volume create myvol
docker volume ls
docker volume inspect myvol
docker volume rm my-vol #But let's not do this for now!

Let's launch an instance connected to the volume!

docker run --name danivolumetest --mount source=myvol,target=/app -d dani:salt_master


From mastering docker:

This is the part that touches on volumes on page 118 of Mastering Docker.

Re-create the network for these containers if it's missing, and then start a redis and web instance:

docker network create moby-counter
docker container run -d --name redis --network moby-counter redis:alpine
docker container run -d --name moby-counter --network moby-counter -p 8080:80 russmckendrick/moby-counter

Stop and remove the redis docker container

docker container stop redis
docker container rm redis

And relaunch it:
docker container run -d --name redis --network moby-counter redis:alpine

All your mobys are gone! :o

This happens due to the redis instance using a container, but every time


7 ----- DOCKERVOLUMES -----


8 ----- DOCKERCMD -----

IMAGES:
docker image build --tag local:debiantest .

docker build . #Creates unnamed image!

docker image inspect local:debiantest #Neat!

CONTAINERS:

List all containers:
docker ps -a

Remove all stopped containers:
docker rm $(docker ps -a -q)

Remove all containers regardless of status in true YOLOMAX fashion:
docker rm $(docker ps -a -q) --force

docker container run -d -p 8080:80 --name apache-php7 local/apache-php:7

MISC / TO REVIEW
docker attach $INSTANCE

DANI'S image:
docker image build . --tag dani:salt_master
docker run -d --network pub_net dani:salt_master


CONTAINERS FROM THE MASTERING DOCKER:

Run hello world:
docker container run hello-world

List your containers:
docker container ls -a

Remove one of them:
docker container rm a5b8567a86c5

This launches a container in the background (-d flag), named nginx test, with the container's 80 port mapped to your localhost 8080 port, using the nginx image
docker container run -d --name nginx-test -p 8080:80 nginx

This does the same thing, but in foreground. You'll have to ctrl+c to get out it, and you'll the the logs on the terminal if you reach nginx on the instance through a browser:
docker container run --name nginx-foreground -p 9090:80 nginx

If you launch these two containers (one in foreground and other in background, and you exit the foreground one by pressing ctrl+c), and then run docker container ls -a, you'll that the foreground one is exited.

ATTACH
You can interact with your container by attaching to it:
docker container attach nginx-test

Ooh! nginx logs show on the terminal. Detaching (ctrl+c) terminates the container though :(

Let's start that container back up:
docker container start nginx-test

Let's attach again, but this time with an extra option:
docker container attach --sig-proxy=false nginx-test

Now when you detach (ctrl+c) the container no longer stops. Neat!

EXEC
Running the attach command is useful if you want to connect to the process your container is running, but the exec one seems more interative!

The exec command spawns a second process inside the container that you can interact with. Examples:
docker container exec nginx-test whoami
docker container exec nginx-test cat /etc/debian_version

These processes run and then terminate, leaving the container as it was before it ran (unless your processes change something!)

We can take it one step further and run bash inside the container!

docker container exec -i -t nginx-test /bin/bash

This way you can interact with the container as if you had ssh'ed into it! It is not recommended to do any modifications to the machine this way, as they'll be lost (remember, containers are ephemeral)

There are two flags for this command
-i: Shorthand for --interactive, which tells docker to keep stdin open so we can send commands to the process
-t: Short for --tty, which allocates a pseudo-TTY to the session

LOGS
This one is neat: It allows you to interact with the stdout stream of your containers, which docker keeps in the background.

docker container logs --tail 5 nginx-test

You can also see logs in real time:
docker container logs -f nginx-test

And you filter them by time:
docker container logs --since 2018-05-09T08:00 nginx-test

TOP
Rather simple, lists processes running inside the container
docker container top nginx-test

STATS
Provides resource usage information for a container of an argument is passed, or all containers of no argument is passed. You can see CPU and memory usage, as well as network usage. Noice!

docker container stats
docker container stats nginx-test

LIMITING RESOURCES
By default when you launch a container, it will have no resource caps. It could use all the resources on the host!

Typically you would limit resources when using the docker container run:
docker container run -d --name nginx-test --cpu-shares 512 --memory 128M -p 8080:80 nginx

However, our container is already running. According to the book, we would limit resource usage on a running container by doing:

docker container update --cpu-shares 512 --memory 128M nginx-test

(On the mastering docker book the author had an error when running the above command... hey it worked on my machine!(TM). I did get a message about my kernel not having the ability to limit swap, but I'm not too worried about that...

Now, upon doing:
docker container stats nginx-test
I can see resource usage limitation in place.

MISC CONTAINER STUFF

PAUSE

for i in {1..5}; do docker container run -d --name nginx$(printf "$i") nginx; done

You can pause containers:
docker container pause nginx1

You can then unpause them:
docker container unpause nginx1

STOP, START, RESTART & KILL
STOP a container. Does the same thing as when sending ctrl+c to a container that is in the foreground. It sends a SIGTEM to the process running in the container. If the process does not terminate gracefully after a while, docker then sends a SIGKILL. This will immediately terminate the process, not giving it time to write to disk or have database queries finish or anything.

docker container stop nginx2

While having a container terminate ungracefully is a bad thing, docker allows you to change the default grace period for a container to exit gracefully:

docker container stop -t 60 nginx3

You can start containers that are stopped. start supports multiple containers as arguments:
docker container start nginx2 nginx3

The restart command is a combination of two: It stops and starts a container.

docker container restart -t 60 nginx4

And finally, in true YOLOMAX fashion, you can send sigkill directly to a container by doing

docker container kill nginx5

REMOVING CONTAINERS

To remove all stopped containers:
docker container prune

But be careful running this, if all your containers are stopped, you will remove all of them!

You can also remove a specific container:

docker container rm nginx4

If you want to go yolomax and remove a running container, you have to use rm -f:
docker container rm -f nginx4

CREATE
Similar to the run command, prepares and configures a container, but doesn't start it.

PORT
Lists port mappings for a given container. This might be more useful than it looks!
docker container port nginx-test

DIFF
Lists differences in the container filesystem since the container started to run. (files created, removed and changed).

If this container is stopped and removed, these files will be lost (but you should be okay with that!)





8 ----- DOCKERCMD -----








----- RANDOM NOTES TO FOLLOW UP LATER -----

Possible workflow:
- Create dockerfile
- Create image (docker image build --tag local:debiantest .)
- Launch container (docker container run -it--name debiantest1 local:debiantest /bin/bash)


docker build --tag local/apache-php:7 .




docker pull vioseven/docker_automated #pull image from dockerhub (created on dockerhub)

Document docker per subcomands (image, container, swarm, volume and so on)

