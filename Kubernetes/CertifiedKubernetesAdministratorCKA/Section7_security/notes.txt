
95: Security primitives
Two things to set up:
Who can access the cluster?
What can they do?

Who can access:
Defined by authentication mechanisms
You can base user access on:
- Files - Usernames and Passwords
- Files - Usernames and Tokens
- Certificates
- External certification providers (such as LDAP)
- Service accounts (for machines only)

What can they do:
Defined by authorization mechanisms.
You can set this up with:
- RBAC Auth (role based access controls)
- ABAC Auth (atribute based access control)
- Node Authorization
- Webhook mode

Communication between applications in the cluster:
By default, all pods can talk to all pods within the cluster.
You can restrict access between them by using network policies.


96: Authentication
We have two types of users: Humans, such as admins and developers
And service accounts, aka robots.

Kubernetes does not manage user accounts natively. It relies on an external source for that, like a file or certificates or external auth service.

You can however, create and manage service accounts through kubernetes with:
kubectl create serviceaccount sa1
kubectl list serviceaccount

All user access is managed by the kube-apiserver (aka api server). It authenticates the request before processing it

Ways that the kube-api server authenticates:
- You can have a list of usernames and passwords on a static password file
- You can have a list of usernames and tokens on a static file
- You can authenticate using certificates
- And you can connect to a third party for auth (such as LDAP)

Users and passwords on file:
For list of usernames and passwords on file, you can create them as a .csv (:o) on the api server and use that as login information:
password123,user1,u0001
password123,user2,u0002
password123,user3,u0003

The list above can also have a fourth column for groups.

You then pass this file to the kube-apiserver on start up: --basic-auth-file=userdetails.csv (if you set kube-api from scratch!)
If you set it up using kube-adm, you have to change the pod definition for the kube-apiserver pod to contain this command like flag

To then authenticate on the kube-apiserver using this type of auth, you have manually pass it by plaintext on a curl request (:ooo)
curl -v -k https://master-node-ip:6443/api/v1/pods -u "user1:password123"

Static token file:
<big long token>,user10,u0010,group1
<big long token>,user10,u0012,group2

When starting kube-apisever, pass: --token-auth-file=user-details.csv
You then pass the token on a curl request with --header "Authorization: Bearer <bigtokenstring>"

Protip: Neither token on plaintext or users on plaintext are recommended to adding new users.
Use Role Based Authentication for new users!

97: Basic auth set up
(Follow up from Dani: If this is interesting, set up kube-apiserver to use basic auth)

98: TLS Basics
Symmetric Encryption: Data is encrypted and decrypted using the same key. Sender and receiver need to have the key, which means it has to be exchanged between sender & receiver.
Someone sniffing the network would be able to get the message and the encryption key.

Assymetric encryption: Uses private and public keys. SSH also uses this communication format.

Generate CSR (Certificate Signing Request)
Send it to the CA (Certificate Authority)
CA signs the CSR and now you have an actual CRT (Certificate)

CAs use public and private keys to sign certificates. The public key of all CAs is built into the web browser.
The browser uses the CA's public key to validate that the certificate was generated by the CA themselves.

These are public CA's that let you validate public websites.
They are not private CA's that can let you validate sites inside your local network / organization

You can have your own private CA (some CA vendors offer a private solution)

Usually certificates that are public keys are named .pem or .crt.
Usually private keys are named .key or -key.pem.
Private keys have the word "key" in them usually, either as an extension or part of the file name.


100: TLS in Kubernetes. Yay for lecture 100! We're halfway there!
Three types of certificates:
Root certificates (from the CA)
Server certificates (from the server)
Client certificates (from your local)

All interaction between the k8s master and nodes is encrypted.
kubectl establishes a secure connection to talk to the kube-apiserver

Server certificates for servers (master?)
Client certificates for clients  (nodes? remote user with kubectl?)
(Follow up from Dani: Clear that up!)

Server components:
kube-apiserver exposes a HTTPS service. It has:
apiserver.crt
apiserver.key
ETCD server also has etcdserver.crt and etcdserver.key
Kubelet server also has kubelet.crt and kubelet.key

Client components, the clients can be:
Us, admins, connecting to the cluster through kubectl or the REST api. (admin.crt, admin.key)
kube-scheduler talks to the kube-apiserver to find pods that require scheduling and then gets the apiserver to schedule the pods on the right worker nodes. As far as the kube-apiserver is concerned, the scheduler is just another client, so it also needs to validate it's identity (scheduler.key, scheduler.crt)
kube-controller-manager (controller-manager.crt, controller-manager.key)
kube-proxy (kube-proxy.crt, kube-proxy.key)
The kube-apiserver is the only service that talks to the ETCD server, so as far as the ETCD server is concernted, the kube-apiserver is also a client.
The kube-apiserver also talks to the kubelets. In both of these communications, it can use api-server.key and api-server.crt, or you can generate certificate pairs for each communication channel.

Kubernetes requires you to have at least one CA to sign all of this!
You can even have more than one, one for all the components in the cluster, and another one for ETCD and apiserver (or just ETCD)
For now we'll just stick to one CA for everything.


101: TLS in Kubernetes: Certificate creation

Let's get started with the CA certs:
Generate key: openssl genrsa -out ca.key 2048
Generate CSR: openssl req -new -key ca.key -subj "/CN=KUBERNETES-CA" -out ca.csr
Sign certificates: openssl x509 -req -in ca.csr -signkey ca.key -out ca.crt
Since this is for the CA, it's self signed with the own key from the CA.

Now let's do it for the admins:
openssl genrsa -out admin.key 2048
openssl req -new -key admin.key -subj "/CN=kube-admin/O=system:masters" -out admin.csr
openssl x509 -req -in admin.csr -CA ca.crt -CAkey ca.key -out admin.crt

Using O=system:masters is important above as it defines the certificate as meant for an admin user

When using kubectl, you can create a file named kube-config.yaml with kind: Config and configure your certificates in there.
We'll look at kube-config.yaml later.

Whenever you configure a server or a client with certificates, you also need to distribute/specify a copy of ca.crt.
The certificate for the kube-apiserver requires a openssl.cnf file as it's referred to by a bunch of names

Each kubelet service also requires it's own certificate, named after it's host machne (ex: node03)


102: View certificate details
It's important to know how the cluster was set up. (The hard way or kubeadm).
Both use different methods to manage certificates
If you deploy it from scratch, you create all the certificates by yourself.
In this example we'll look at a cluster provisioned by kubeadm.

In an environment set up by kubeadm, you can find the certificates for the kube-api server in the pod definition under
/etc/kubernetes/manifests/kube-apiserver.yaml

To decode a certificate and view details:
openssl x509 -in /etc/kubernetes/pki/apiserver.crt -text -noout

If kubernetes is broken for some reason and you can't use kubectl logs to see the logs, you have to go one later down and use docekr to see the logs (docker logs mycontainer)

(Follow up from Dani: There's definitely some learning to be done as far as troubleshooting a broken cluster (as in when you can't use kubectl)


105: Certificates API
Kubernetes has an API call that can sign CSRs for us!
We can create a kubernetes object named CertificateSigningRequest.
pacman is the package m
Let's create all the things for a user namedpacman is the package m Jane.
openssl genrsa -out jane.key 2048
openssl req -new -key jane.key -subj "/CN=jane" -out jane.csr
See 105_CSR.yaml for an example of a CertificateSigningRequest object (syntax might be a bit off, didn't test it)
kubectl apply -f 105_CSR.yaml
kubectl get csr
kubectl certificate approve jane
kubectl get csr jane -o yaml #Certificate will be part of the output

All certificate related operations are carried out by the controller manager.

107: Certificate quiz:
(Follow up from Dani: Can you generate a CertificateSigningRequest from memory? It's a tough one)
kubectl certificate approve akshay
kubectl certificate deny agent-smith


107: KubeConfig
By default, kubectl looks for a file with the path $HOME/.kube/config which will contain the adress of the cluster and your login credentials
The config file has 3 sections: Clusters, Contexts and Users.
Clusters: Has configs related to the different clusters you might have access to (dev, prod, gcloud, whatever)
Users: The user accounts with which you have access to these clusters. These users might have different privileges on different clusters
Contexts: Defines which user account will be used to acccess each cluster. ex: dev@google, admin@production and so on.

The server specification (address:port) goes into the Clusters section.
The keys and certificates go into the users section.
You then create a context specifying to use certain credentials on a certain cluster.
You can specify a default context on kubeconfig.

Have a look at 107_kubeconfig.yaml for an example. It's pretty straightforward.

You can use kubectl to view and modify the config file too.
kubectl config view
kubectl config view --kubeconfig=my-custom-config
kube config use-context prod-user@production #This will reflect on the config file

You can update your config file using other variations of the kubectl command:
kubectl config -h

You can also specify a namespace under a context definition in config.
You can have a path to a certificate for a cluster in the config, or you can have the certificate in base64 in the config.


110: API Groups
Whatever operations we've done so far to the cluster, we've been interacting with the API server, one way or the other.
For instance, to get the version of your cluster, you can:
curl https://kube-master:6433/version
To get a list of pods you can:
curl https://kube-master:6433/api/v1/pods

The Kubernetes API is grouped into multiple endpoints depending on their purpose:
(/metrics, healthz, /version, /api, /apis, /logs and so on)
(Follow up from Dani: Can you investigate all API endpoints and interact with them later?)
In this section, we focus on the APIs responsible for the cluster functionality, which are /api and /apis
Core API: /api
named API: /apis

The core group is where all core functionality exists
(v1/namespaces, v1/pods, v1/rc, v1/events, v1/endpoints, v1/nodes, v1/secrets and a bunch of other endpoints)

The named group /apis is slightly more organized (according to author) and moving forward all k8s functionality will be here.
(/apps, /extensions, /storage.k8s.io, /authentication.k8s.io, /certificates.k8s.io)

Within apps you have:
/apps/v1/deployments
/apps/v1/replicasets
/apps/v1/statefulsets

And a bunch more or endpoints!

Each resource (like /apis/apps/v1/deployments) has a set of actions associated with them (such as list, get, create, delete, update, watch)
The API reference can tell you what the API group is for each object.

You can also access your API to see the supported API groups:
curl http://localhost:6443 -k
curl http://localhost:6443/apis -k | grep "name"

A note on using curl to directly access the API: You have to authenticate! So you'll probably have to do something like:
curl http://localhost:6443 -k --key admin.key --cert admin.crt --cacert ca.crt

You can also do this: kubectl proxy
Which will start a proxy on 127.0.0.1:8001, using your certificates and config files to connect to the cluster for you.
You can then curl this proxy and it will be the same as curling your target cluster, but already authenticated and with proper permissions!

However, do note: kube proxy is NOT the same as kubectl proxy

In the next section, we can see how we use parts of the API and verbs to allow or deny access to users.


111: RBAC
First of all, we create a role object. Please see 111-developer-role.yaml. You then apply the role:
kubectl apply -f 111_developer-role.yaml
When we have our role object set up, we need to create a role binding. See 111_devuser-developer-binding.yaml
Do note that these apply to the default namespace.
(Follow up from Dani: How to create other users in other namespaces?)

To view roles and role bindings:
kubectl get roles
kubectl get rolebindings
kubectl describe role developer
kubectl describe rolebinding devuser-developer-binding

You can also use this:
kubectl auth can-i create deployments
kubectl auth can-i delete nodes

That's pretty cool! It'll return a boolean saying if you can or can not execute the specified action
You can even test an user's permission:

kubectl auth can-i create deployments --as dev-user
kubectl auth can-i create pods --as dev-user
kubectl auth can-i create pods --as dev-user --namespace test

You can even go one level down and restrict someone's access to something with certain names. For instance, you could restrict a role to interact only with the pods named "blue" and "orange"
See 111_resource-names.yaml

112: Practice test: RBAC
(Follow up from Dani: When confused, can I query the API directly to obtain answers?)
(Follow up from Dani: Can you create role binding from memory? Can you accurately copy and paste one from the system?)
(Follow up from Dani: It got really boring copy and pasting roles and rolebindings around. Do some exercises on these subjects later if you feel like it)


113: Cluster Roles.
There are Cluster Roles and Cluster Role bindings! Uh-oh!
Roles and RoleBindings are namespaced, meaning they're created within namespaces.
If you don't specify a namespace, they're created in the default namespace.
Namespaces as we've been seeing until now, can isolate some resources, but not all of them!
You can't divide nodes in namespaces and we've been seeing until now. Those are cluster wide or cluster scoped resources. They cannot be associated to any particular namespace.

Resources are categorized as: Namespaced or Clusterscoped

Namespaced resources: pods, replicasets, jobs, services, secrets, configmaps (the ones we've seen so far!)
Cluster scoped resources: nodes, PVs, clusterroles, clusterrolebindings, certificatesigningrequests, namespaces)

To see the full list of what is namespaced and what is cluster scoped, do:
kubectl api-resources --namespaced=true
kubectl api-resources --namespaced=false

You can use cluster roles to give users to things like nodes and persistent volumes.
See 113_cluster-role.yaml for an example of a cluster role.
And 113_cluster-role-binding.yaml for an example of a binding.

They're very similawr to the namespaced roles and rolebindings.


114: Practive: Cluster roles and cluster role bindings
kubectl get clusterroles --all-namespaces
(Follow up from Dani: You skipped the cluster role binding practice session. Do it later if you're feeling like it)


115: Image security
When you specify on a pod:
image:nginx
Kubernetes actually interprets this as:
image: docker.io/nginx/nginx

To pull an image from a private registry, first you have to create a secret with the login credentials for the registry:

kubectl create secret docker-registry regcred \
    --docker-server=private-registry.io \
    --docker-username=myuser \
    --docker-password=mypassword \
    --docker-email=myemail
(Follow up from Dani: Can you create this secret from memory?)


117: Security Contexts
You can ask a pod to run as a certain user id. Check 117_seccontext_pod.yaml.
This is supported to be set for all the containers in the pod, or you can set it on an individual container in the pod.
Depends on which part of the spec you put it.
Capabilities apparently allows you to give certain priviledges to the process running inside the pod, but not root rights.

If you don't specify a user for a container (either during build or at launch for k8s) it'll run as root

118: Practice test, security policy.
Set the policy for the entire pod but for some reason it didn't get picked up. Tried to put it inside a container and it crashed.
(Follow up from Dani: If you re-do this with the user specified under a container only (not the entire pod) you should be able replicate the failure. Maybe your understanding of yaml isn't that good!)


119: Network Policy
From a webserver, traffic coming from users is an Ingress traffic.
Traffic leaving the cluster is of type egress.

To recap in Kubernetes:
Each Pod has an ip address.
Each node has an ip address.
Each service has an ip address.

By default, all pods on the cluster can reach each other through the Kubernetes virtual networks.
k8s is configured by default with an "All Allow" rule

Imagine you have this:

Frontend <> API server <> Database.

Your frontend should not (and has no reason to) connect to your database directly. It has to go through your backend API first.
You can implement a network policy to allow traffic to the database only from the backend server.
A NetworkPolicy is another object in a k8s cluster.
You link a NetworkPolicy to one or more pods.

We use labels and matchLabels to link pods to policies.

Not all network solutions support network policies.

(Follow up from Dani: The sample yaml given on this one was very crappy. Research and redo the text exercises on lecture 120!)