
61: Rolling updates and rollbacks
When you first create a deployment, it triggers a rollout.
A new rollout creates a new deployment revision. Let's call it rev1.
When the container version is updated (new image?) a new rollout is triggered, generating a new revision, which we can call rev2.

This allows us to keep track of the changes made to our deployments and enables us to rollback if necessary.

You can see the rollout status of a given deployment by doing:
kubectl rollout status deployment/myapp-deployment

To see the revisions and roll out history:
kubectl rollout history deployment/myapp-deployment

There are two type of deployment strategies.
Recreate strategy: All the pods in the deployment are destroyed, and then recreated.
This is disadvantageous as it causes downtime in between the time all the containers are destroyed and the new ones are spawning up.

Rolling update: Imagine 5 pods. Rolling update takes down 1 pod with the old version, and spawns up a new pod with the new version.
It keeps repeating this process until all the pods are running the new version. This is the default strategy.

To change the image version of your deployment, you would simply change the it on the deployment definition yaml and do a:
kubectl apply -f mydeployment.yaml

But you can also do it imperatively:
kubectl set image deployment/myapp-deployment nginx=nginx:1.9.1

When using kubectl describe deployment mydeployment, if using the Recreate strategy, you will see the underlying ReplicaSet being scaled down to 0, and then scaled up to whatever number it had previously.
When using the RollingUpdate strategy, the old replicaset will be scaled down by 1, while the new replicaset will be scaled up by 1 over time.
This process will repeat itself until all pods are on the new replicaset.

To rollout a deployment, you can:
kubectl rollout undo deployment/myapp-deployment

You can see the new and old replicasets both change the number of replicas to reflect the undo change (pods will be gradually destroyed on the new and started on the old)

Command summary:
kubectl -f create mydeployment.yaml
kubectl get deployments
kubectl apply -f mydeployment.yaml #With a new image version
kubectl set image deployment/mydeployment nginx=nginx:1.9.1
kubectl rollout status deployment/myapp-deployment
kubectl rollout history deployment/myapp-deployment
kubectl rollout undo deployment/myapp-deployment


62: Practice Test: Rolling Updates
Memorize how to do this:
kubectl set image deployment/frontend simple-webapp=kodekloud/webapp-color:v2

Remember that except for pods, you can edit all object types on the fly!


64: Commands:
Commands and arguments on a pod definition file. Not strictly part of the certification, but author believes it's important!

In case of ENTRYPOINT, whatever you specify as an argument, will get appended to the entrypoint.
In case of CMD, the command parameters passed will get replaced entirely.

You can also use ENTRYPOINT and CMD like:

FROM Ubuntu
ENTRYPOINT ["sleep"]
CMD["5"]

The default argument for sleep will be 5, but you can overwrite it by passing an argument to the container at launch time.

You can also do something like:
docker run --entrypoint sleep_v2 ubuntu-sleeper 10


65: Commands and Arguments
In a pod definition:
Command is analogous to ENTRYPOINT
args is analogous to CMD


67: Configure Environment Variables in applications
You can set environment variables as a key-value under env on your deployment/pod/etc yaml definition
You can also set them on a configmap or a secret


68: Configuring configmaps in applications
Having too many environment variables set up on a pod definition file will become unwieldy very quickly (duplication over all files & a lot of repeating yourself)
We can take that out of the pod definition and put it on a ConfigMap! :D

There are two phases when creating a configmap:
1. Create the configmap and define variables you want
2. Inject these variables into the pod

Just like any Kubernetes object, there are two ways of creating a configmap.
Imperative: kubectl create configmap
Declarative: kubectl create/apply -f

Creating a configMap imperatively:
kubectl create configmap myconfigname --from-literal=mykey=myvalue
kubectl create configmap myconfigname --from-file=pathtomyfile

kubectl get configmaps
kubectl describe configmaps
kubectl describe configmap myconfigmap


69: Practice test: Environment variables
You can't edit environment variables of a running pod :(
(Follow up from Dani: Creating a pod loading the configs from a configmap was really tough. You need to work on this!


70: Configure secrets in applications
Secrets are very similar fom configmaps, except they're stored on a encoded format

You can also create a secret imperatively and declaratively:
kubectl create secret generic mysecret --from-literal=mysecret=myvalue --from-literal=mysecret1=myvalue1
kubectl create -f

To generate a secret to put on a secret file, do:
echo "mysecret" | base64

To decode the secret:
echo "bXlzZWNyZXQK" | base64 --decode

To see secrets:
kubectl get secrets

To see the actual values of the secrets:
kubectl get secret mysecret -o yaml


71: A note about secrets
Author explains that the way secrets are set up in Kubernetes isn't the safest or most coherent option.
(Follow up from Dani: This page is suggested as further reading: https://kubernetes.io/docs/concepts/configuration/secret


72: Practice test - Secrets
(Follow up from Dani: Why did the test accept a secret created imperatively, but not descriptively? What did you mess up? :thinking:)
kubectl create secret generic db-secret --from-literal=DB_Host=sql01 --from-literal=DB_User=root --from-literal=DB_Password=password123

(Follow up from Dani: Try adding secrets to other objects other than pods, such as Deployments and DaemonSets)