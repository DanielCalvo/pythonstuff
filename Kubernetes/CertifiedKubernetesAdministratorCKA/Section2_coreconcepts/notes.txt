
----- CHAPTER 2: CORE CONCEPTS ----

7:
Core concepts: Cluster architecture from a high level, API primitives, and Services and other Network Primitives


9: Cluster architecture
Cluster consists of a set of nodes. There are worker nodes and master node.
Worker node: Hosts applications as containers
Master node: Manages, plans, executes and monitors nodes

etcd: Information about worker nodes and what containers they have running with additional info is stored in etcd in master
kube-scheduler: Identifies the right node to place a container on based on assorted rules/resources
controller-manager, node-controller, replication-controller: Follow up on these later
kube-apiserver: Responsible for orchestrating all operations within the cluster, exposes the kubernetes API.
Container run time engine: What runes the containers, usually docker, but kubernetes supports others

kubelet: Agent that runs on every node in the cluster, listens from instructions from the kube-api server and handles containers on this node as required
The kube-api server periodically gets status reports from the kubelet to monitor the status of nodes and their containers

kube-proxy: Ensures that the necessary rules are in place to allow the containers running in them to reach each other

Master: ETCD cluster, kube-apiserver, Kube Controller Manager, kube-scheduler
Worker: kubelet, kube-proxy, Container Runtime Engine


10: ETCD for beginners
ETCD is a distributed key-value store.
Listens on port 2379 by default

Really easy to run it, you can download a release from github, uncompress it and launch ./etcd without any other dependencies (it's a go binary)
You can then use ./etcdctl to:
./etcdctl set key1 value1
./etcdctl get key1


11: ETCD in Kubernetes
The ETCD datastore in Kubernetes stores: Nodes, Pods, Configs, Secrets, Accounts, Roles, Bindings & Others
Every information that you get when you type kubectl get comes from ETCD.
Any change done to the cluster is updated to the ETCD server.

If you install k8s from scratch, you launch etcd by hand.
If you use kube-adm to launch kubernetes, it'll launch etcd as a pod in the kubernetes namespace


12: Kube-API Server
When you run a kubectl command, it reaches for the kube-api server. The Kube api server authenticates the request and then validates it. It then communicates with the ETCD cluster and replies to the kubectl command line tool.
You don't need to use kubectl, you could invoke the API directly

Kube API server functionalities:
- Authenticates Users
- Validates requests
- Reads / writes from ETCD. kube-api is the only component that interacts directly with ETCD.
- The scheduler, kubelet and kubectl use the Kube API to perform updates in the cluster.

TODO: Later if you need a refresh on the theory, review Section 2: Core Concepts as I didn't write every single thing down!


13: Kube controller manager:
As the name might indicate, manages other controllers inside the Kubernetes cluster
In Kubernetes terms: A controller is a process that continually monitors the state of various components within the system and works towards bringing the system to a desired state.

Node controller: Responsible for monitoring the states of the nodes. It takes the necessary actions through the kube API server to make sure pods are allocated to healthy nodes, and removes nodes that do not respond to heartbeats under certain circumstances

Replication controller: Responsible for monitoring the states of replicasets, ensuring that the set number of pods are available at all times in the set.

There are many more other controllers. They're all packaged into a single process known as the Kubernetes controller manager.


14: Kube scheduler
Responsible for scheduling pods on nodes. Only responsible for deciding which pod goes on which node. But it doesn't actually place the pod on the node! :o
The kubelet is the controller that creates the pod on the node. The scheduler only decides which pod goes where.

The scheduler on which nodes the pods are placed on. Pods can have different requirements, and nodes can have different ammounts of resources vailable.
The scheduler looks at each pod and tries to find the best node for it.


15: Kubelet
Runs on worker nodes. Registers the worker node in the cluster. When it receives a request to start a pod, it launches it by communicating with the Container runtime engine (Docker in most cases) to pull the required image and run an instance. Monitors the status of the pod and containers in it and reports it to the kube api server.
Kubeadm does not deploy kubelets. You must always manually install Kubelet.


16: Kube proxy
Inside a cluster, all pods can communicate among themselves. This is accomplished by deploying a pod networking solution to the cluster.
A POD network is a virtual network that all the pods connect to and it spans across all the nodes in the cluster

Kube proxy implements things like service IPs, which are virtual IPs. It uses IPTables rules to make sure that the traffic that his a virtual IP assigned to a service actually reaches a pod.


17: Recap - Pods
Containers are incapsulated in a Kubernetes object known as Pod. A pod is a single instance of an application. A pod is the smallest object that you can create in Kubernetes.
To scale up an application, you bring up new pods. Pods from the same app can be deployed on the same or different nodes.

You can have multiple containers in a pod. You can have a helper container that needs to scale up with the app. This helper container will scale up and down together with your app, as it's on the same pod. Containers on the same pod can communicate with each other by referring to themselves by localhost. They share the same network space.
kubectl run nginx --image nginx
kubectl get pods


18:
How many pods exist in the system?
kubectl get pods
0

Create a new pod with the nginx image
kubectl run nginx --image nginx
OK

What is the image used to create the new pod?
kubectl describe pod nginx-64f497f8fd-55lq4
nginx
eh, question was actually referring to the busybox pod, blergh

Which nodes are these pods placed on?
(Follow up from Dani: How do you get a single spec of information with kubectl?)
(Follow up from Dani: Learn how to get single bits of information from commands by formatting them. grep is a bit unrefined)

How many containers are part of pod X?
In this case it would've been two but one of the containers failed to pull. It can be a trick question though, technically it is two pods, but one failed to start
kubectl get pods shows you the number of containers in a pod? Confirm that, find a way to do that quickly too! I think kubectl get pods shows this

What does the READY column in kubectl get pods mean?
(Follow up from Dani: What does the default output on every single kubectl command mean? That's gonna be a good one!)

Delete the webapp pod:
kubectl delete webapp

Create a new pod with the name redis and the image redis123. Use a pod definition yaml.
Holy shit I gotta create/copypaste/adapt stuff on the fly! :o

See 19_redispod.yaml

kubectl apply -f 19_redispod.yaml

(Follow up from Dani: Check out the kubectl edit command)


19: Recap - ReplicaSets
A controller is a process that monitors Kubernetes objects and responds accordingly
There is a controller named the replication controller
The replication controller allows us to run multiple instances of the same pod in the cluster, providing high availability. (If you have a single pod and it dies, bad luck)
Even if you have a single pod, a replication controller can be useful by bringing up a new pod of the current one fails.

The replication controller allows you to have multiple pods to share the load across them.

A Replication Controller is not a Replica Set. The Replication Controller is older, but the concepts apply to both of these.

In the 21_replicationcontroller.yaml definition, you have the following entries:
apiVersion
kind
metadata
spec

Inside the spec you have a template, and in there you actually have a pod definition (minus apiVersion and spec).
That replication controller yaml did not work! :o It's missing a selector. Maybe the course is from an older version of Kubernetes?
(Follow up from Dani: Ooooh! apiVersion actually tells you to which version of the Kubernetes API to talk to! :o)

Aaah, replicaSet requires a selector. It appears this was also applied to ReplicationController.
It appears in the past a selector was not required in a ReplicationController, but now it is!

Why the selector on the replica set? It's because ReplicaSets can manage pods that were not created as part of the ReplicaSet creation.
If you had some pods created before you created the ReplicaSet with the matching labels, ReplicaSet can take care of those too!

(Follow up from Dani: kubectl create returns more useful error messages than kubectl apply)

kubectl get replicaset
kubectl get pods

ReplicaSet knows what pods to monitor based on labels.

(Follow up from Dani: When updating an existing object, you use the kubectl replace -f command to update an existing definition)

When updating the number of replicas of a given replicaset, you can use the kubectl replace -f to update it.
You can also do:
kubectl scale --replicas=6 21_replicaset.yaml
kubectl scale --replicas=5 replicaset danireplica

You can also scale up replicasets based on load, which will be discussed later!

To delete a given replicaset:
kubectl delete replicaset danireplica
Do not that this will delete the pods managed by the replicaset!

Command recap:
kubectl create -f rc.yaml
kubectl get replicaset
kubectl delete replicaset danireplicaset
kubectl replace -f rc.yaml
kubectl scale --replicas=6 -f rc.yaml

(Follow up from Dani: See if you can use all of the commands above without the -f flag)


22: Practice test: ReplicaSets
(Follow up from Dani: Can you write a replicaset definition from memory?)
(Follow up from Dani: Learn how to use the edit command to get running resources and make a copy from them)


24: Deployments
Deployments allow you to have several updates strategies to your pod images (rolling updates, undo changes, pause and resume changes as required)
The syntax for a deployment type appears to be exactly the same as the ReplicaSet, the only difference being that it is of type "Deployment"

kubectl applf -y deployment.yaml
kubectl get deployments
kubectl get replicaset
kubectl get pods

Do note that the deployment object will create an underlying replicaset object.

"kubectl get all" will show you everything in the cluster.


25: Certification tip:
(Follow up from Dani: Have a look at this later!)
(Follow up from Dani: You have an object in a k8s cluster. How can create a .yaml file out of it?. You should have a look at kubectl run and the cheatsheet https://kubernetes.io/docs/reference/kubectl/conventions/)
Working example of kubectl run:
kubectl run nginx --image=nginx --replicas=5


26: Practice test: Deployment
(Follow up from Dani: Can you create a kubernetes .yaml validator that you can copy and paste things into? :thinking:)
Protip: You can apply/create the .yaml files to attempt to figure out what's wrong


27: Namespaces
There is the default namespace, which is where I assume all your objects go to when you just get a cluster and start launching things.
There is also the kube-system namespace, which is where Kubernetes run it's own components.
There is also the kube-public namespace, which is where resources managed by kubernetes but that should be acessible by the public are created

If you environment is small, you don't really have to worry about namespaces. You can do everything on the default one.
You can have "dev" and "prod" namespaces, which will isolate the resources between them.
You can also assing resource quotas/limites to namespaces.
The apps inside a namespace can reffer to themselves simply by their names. If you have a pod named "mydb" inside your namespace, you chould reach it by simply using the name "mydb" (ex: ping mydb)
You apps can also reach apps in another namespaces. Imagine you have a dev123 namespace. To connect to mydb on it, you would do:
"ping mydb.dev.svc.cluster.local"
When a service is created on a cluster, a DNS entry is created in the format of:
mydb.dev123.svc.cluster.local
mydb: Service name
dev123: namespace
svc: service
cluster.local: domain

kubectl get pods will only list pods in the default namespace.
If you wanted to list pods in another namespace, you have to specify it:
kubectl get pods --namespace=kube-system

If you want to create a pod from a .yaml file in another namespace, you can use the --namespace option:
kubectl create -f pod-definition.yaml --namespace=dev
You can also have the namespace set up inside the .yaml file. See 27_podnamespace.yaml to see the namespace entry, it's just a line.

You can specify a namespace under a .yaml file, just like any k8s object.
You can also: kubectl create namespace dev

You can set the default namespace that kubectl is interacting with with:
kubectl config set-context $(kubectl config current-context) --namespace=dev

To view a given resource on all namespaces:
kubectl get pods --all-namespaces

You can also create a resource quota for a namespace. Take a look at 29_resourcequota.yaml


28: Practice test: Namespaces
kubectl get namespaces
kubectl get pods --namespace=research


29: Services
Services are used to enable connectivity between pods and external data sources.
Services are objects, just like pods and replicasets.

A NodePort service will listen on a given port on the node, and forward that traffic to a pod.
A ClusterIP service will create a virtual IP inside the cluster to enable communication between a set of services.
A LoadBalancer service (???) was explaining badly. Implies external connection apparently.

NodePort explanation:
- The port on the pod in which the application is 80, and it's referred to as the "target port"
- The second port is the port on the service itself, and it's simply referred to as "port"
- There is also the port on the node itself, used to access webserver internally, referred to as "nodeport". The valid port range for a node port is between 30000 to 32767

Out of these, the only mandatory field is "Port".
If you do not provide a targetPort, it is assumed to be the same as Port.
If you do not provide a nodePort, a free one in the valid range (30000 - 32767) will be automatically alocated.
Note on the yaml definition that ports is an array. You can have multiple port mappings in the same service.

To link a service to pods, you use a selector. This will match against labels on pods.

Using the service on 29_servicedefinition.yaml:

kubectl create -f 29_servicedefinition.yaml
kubectl get services

(Follow up from Dani: Carefully study the output of kubectl get services to know exactly what everything means. Give that a go on kubectl describe services too)

Remember: The service acts as a built-in load balancer. It loads balance randomly between pods.
A service spawns across all the nodes and pods, mapping everything to the ports defined in the service definition.


30: Kubernetes service: ClusterIP
Pods have an IP adress assigned to them, but when you have a deployment handling those pods, these IPs change very often, as pods are ephemeral.
You cannot rely on these IP addresses for internal communication.
A K8s server can group the pods together with a Virtual IP and provide a single "interface" to reach these pods.
ClusterIP is the default type on Kubernetes services (if you don't specify a service type on your yaml, you get a cluster IP)

On the yaml spec: The target port where the backend is exposed in the pods. The port is which port in the service this ports gets exposed.


31: Practive test - Services
Note: When doing kubectl get services, the port that you will see in the PORT(S) column is the service port, NOT the targetPort.
You can see the targetPort by describing the service.


32: Certification Tip: Imperative commands with kubectl
--dry-run: This will not create a service for you, but will help you validate the .yaml file.
-o yaml: This will output the resource definition of your object as yaml in the screen

To generate a config for a pod:
kubectl run --generator=run-pod/v1 nginx --image=nginx --dry-run -o yaml

To create a deployment:
kubectl run --generator=deployment/v1beta1 nginx --image=nginx (old way)
kubectl create deployment --image=nginx nginx (newer way)

To generate yamls for deployments:
kubectl run --generator=deployment/v1beta1 nginx --image=nginx --dry-run -o yaml
kubectl create deployment --image=nginx nginx --dry-run -o yaml

kubectl run --generator=deployment/v1beta1 nginx --image=nginx --dry-run --replicas=4 -o yaml

You can then save the definition to a file:
kubectl run --generator=deployment/v1beta1 nginx --image=nginx --dry-run --replicas=4 -o yaml > nginx-deployment.yaml

To generate service definitions:
kubectl expose pod redis --port=6379 --name redis-service --dry-run -o yaml


kubectl create service clusterip redis --tcp=6379:6379 --dry-run -o yaml
kubectl create service nodeport nginx --tcp=80:80 --node-port=30080 --dry-run -o yaml

(Follow up from Dani: You should definitely follow up on the creation of yaml definitions from the command line. It could be very useful! Note that for some things you need to run run (pod) and for others you need to do expose (service)
follow up on imperative commands for all covered object types for the certification)


33: Practice test: Imperative commands
Very useful: https://kubernetes.io/docs/reference/kubectl/conventions/

Deploy a nginx pod using the nginx:alpine image
kubectl run nginx-pod --image=nginx:alpine --generator=run-pod/v1

Create a redis pod with the label tier = db
kubectl run redis --image=redis:alpine --generator=run-pod/v1 --labels=tier=db

Create a service redis-service to expose the redis application within the cluster on port 6379.
kubectl expose pod redis --port=6379 --name redis-service
The above creates a ClusterIP service. Nice! :D

Create a deployment named webapp using the image kodekloud/webapp-color with 3 replicas
kubectl create deployment webapp --image=kodekloud/webapp-color #But how do you scale it?
kubectl run --generator=deployment/v1beta1 webapp --image=kodekloud/webapp-color --replicas=3 #Worked this way!

Expose the webapp as service webapp-service application on port 30082 on the nodes on the cluster. The web application listens on port 8080
Meh this one sucked. It told me to create a .yaml with an imperative command and then edit it. That defeats the purpose of using imperative commands!


